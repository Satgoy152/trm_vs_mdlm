{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f0691b",
   "metadata": {},
   "source": [
    "# Test Inference for MDLM\n",
    "\n",
    "This notebook loads a trained MDLM model from `outputs/mdlm/checkpoints/final`, performs iterative masked inference, and visualizes the top logits and entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69742b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add src to path\n",
    "if \"src\" not in sys.path:\n",
    "    sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "\n",
    "from src.mdlm import MDLM\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Configuration\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Model Config:\", config[\"model\"])\n",
    "\n",
    "# Initialize Model\n",
    "model = MDLM(config)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load Checkpoint\n",
    "checkpoint_path = \"outputs/mdlm/checkpoints/final\"\n",
    "# Try to find the checkpoint file\n",
    "if os.path.isdir(checkpoint_path):\n",
    "    # Check for various formats\n",
    "    possible_files = [\"pytorch_model.bin\", \"model.safetensors\", \"model.pt\"]\n",
    "    found = False\n",
    "    for fname in possible_files:\n",
    "        p = os.path.join(checkpoint_path, fname)\n",
    "        if os.path.exists(p):\n",
    "            print(f\"Loading weights from {p}\")\n",
    "            # Use torch.load for .bin/.pt, safetensors requires loading differently usually but accelerate might save as .bin\n",
    "            if fname.endswith(\".safetensors\"):\n",
    "                from safetensors.torch import load_file\n",
    "                state_dict = load_file(p)\n",
    "            else:\n",
    "                state_dict = torch.load(p, map_location=device)\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        # Maybe it's an accelerator checkpoint with subfolders\n",
    "        print(f\"Warning: No standard model file found in {checkpoint_path}. Attempting to load using accelerate if needed or user intervention required.\")\n",
    "        # If it was saved with accelerator.save_state(), it might have multiple files.\n",
    "        # But here we assume a gathered checkpoint or similar.\n",
    "        # As fallback, just try loading 'pytorch_model.bin' even if os.path.exists failed (symlinks?)\n",
    "        pass\n",
    "else:\n",
    "    print(f\"Checkpoint path {checkpoint_path} does not exist. Please check path.\")\n",
    "    # For demonstration, we proceed with random weights if file missing, BUT user asked to load it.\n",
    "    # We'll just define state_dict variable to None if not found\n",
    "\n",
    "if 'state_dict' in locals():\n",
    "    # If the state dict has 'model' key (common in some lighting/accelerate saves)\n",
    "    if \"model\" in state_dict:\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    \n",
    "    # MDLM wraps modules, keys might need adjustment\n",
    "    # current model has: embeddings, backbone, output_head\n",
    "    # state_dict should match\n",
    "    keys = model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"Model loaded with keys:\", keys)\n",
    "else:\n",
    "    print(\"Skipping weight loading (file not found). Using random initialization for demo.\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"data\"][\"tokenizer\"])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fa645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_and_entropy(model, input_ids):\n",
    "    \"\"\"Run forward pass and compute logits and entropy.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # MDLM structure: embeddings -> backbone -> output_head\n",
    "        # We access components directly since MDLM.forward is for training loss\n",
    "        \n",
    "        # 1. Embeddings\n",
    "        x = model.embeddings(input_ids)\n",
    "        \n",
    "        # 2. Backbone\n",
    "        # Create attention mask (all 1s if not padding)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        hidden = model.backbone(x, attention_mask)\n",
    "        \n",
    "        # 3. Output Head\n",
    "        logits = model.output_head(hidden) # [B, L, V]\n",
    "        \n",
    "        # Compute Entropy\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        entropy = -(probs * log_probs).sum(dim=-1) # [B, L]\n",
    "        \n",
    "    return logits, entropy\n",
    "\n",
    "def print_top_k_for_token(logits, tokenizer, token_idx, top_k=10):\n",
    "    \"\"\"Print top K predictions for a specific token position.\"\"\"\n",
    "    token_logits = logits[0, token_idx]\n",
    "    probs = F.softmax(token_logits, dim=-1)\n",
    "    \n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    print(f\"Top {top_k} for position {token_idx}:\")\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"  {token!r}: {prob:.4f}\")\n",
    "\n",
    "def visualize_entropy(entropy, tokens, title=\"Entropy per Token\"):\n",
    "    \"\"\"Plot entropy per token position.\"\"\"\n",
    "    entropy_np = entropy[0].cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.bar(range(len(entropy_np)), entropy_np)\n",
    "    plt.xticks(range(len(entropy_np)), tokens, rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Entropy (nats)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def mask_text(input_ids, mask_token_id, mask_prob=0.15):\n",
    "    \"\"\"Randomly mask tokens.\"\"\"\n",
    "    # Create mask\n",
    "    mask = torch.rand(input_ids.shape) < mask_prob\n",
    "    masked_input = input_ids.clone()\n",
    "    masked_input[mask] = mask_token_id\n",
    "    return masked_input, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32327e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Inference\n",
    "initial_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "mask_probability = 0.3 # Mask X% of tokens each step\n",
    "num_steps = 3\n",
    "mask_token_id = 50256 # From config or tokenizer.eos_token_id usually 50256 for GPT2\n",
    "\n",
    "# Prepare Input\n",
    "input_ids = tokenizer.encode(initial_text, return_tensors=\"pt\").to(device)\n",
    "current_ids = input_ids.clone()\n",
    "\n",
    "print(f\"Original Text: {initial_text}\")\n",
    "print(f\"Input IDs: {current_ids}\")\n",
    "\n",
    "# Iterative Loop\n",
    "for step in range(num_steps):\n",
    "    print(f\"\\n{'='*20} Step {step+1} {'='*20}\")\n",
    "    \n",
    "    # 1. Mask Tokens\n",
    "    # We mask a random subset of tokens\n",
    "    masked_ids, mask_bool = mask_text(current_ids, mask_token_id, mask_probability)\n",
    "    masked_ids = masked_ids.to(device)\n",
    "    \n",
    "    masked_text = tokenizer.decode(masked_ids[0])\n",
    "    print(f\"Masked Input: {masked_text}\")\n",
    "    \n",
    "    # 2. Forward Pass\n",
    "    logits, entropy = get_logits_and_entropy(model, masked_ids)\n",
    "    \n",
    "    # 3. Visualization\n",
    "    # Visualize entropy\n",
    "    tokens_str = [tokenizer.decode([t]) for t in masked_ids[0]]\n",
    "    visualize_entropy(entropy, tokens_str, title=f\"Entropy Step {step+1}\")\n",
    "    \n",
    "    # Show Top 10 for masked positions (just the first few masked ones to save space)\n",
    "    masked_indices = torch.where(mask_bool[0])[0]\n",
    "    if len(masked_indices) > 0:\n",
    "        print(\"Predictions for masked positions:\")\n",
    "        for idx in masked_indices[:3]: # Show max 3\n",
    "            print_top_k_for_token(logits, tokenizer, idx)\n",
    "    else:\n",
    "        print(\"No tokens were masked this step.\")\n",
    "\n",
    "    # 4. Sampling / Update\n",
    "    # We sample from the model's output to fill the masks\n",
    "    # For unmasked tokens, we keep the original (or input)\n",
    "    # This loop logic assumes we want to evolve the text\n",
    "    \n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    sampled_ids = torch.multinomial(probs.view(-1, probs.size(-1)), 1).view(probs.size(0), -1)\n",
    "    \n",
    "    # Update current_ids: Keep unmasked, replace masked with sample\n",
    "    # Note: mask_bool matches masked_ids\n",
    "    current_ids = torch.where(mask_bool, sampled_ids, masked_ids)\n",
    "    \n",
    "    decoded_text = tokenizer.decode(current_ids[0])\n",
    "    print(f\"Output Text: {decoded_text}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
